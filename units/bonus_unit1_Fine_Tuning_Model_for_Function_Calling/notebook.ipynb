{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Bonus Unit 1: Fine-Tuning a model for Function-Calling](https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the dependencies & Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U trl\n",
    "!pip install -q -U tensorboardX\n",
    "!pip install -q wandb\n",
    "!pip install -q -U torchvision\n",
    "!pip install -q -U transformers\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/Playground/study-group-hf-agents-course/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, set_seed\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "import os\n",
    "\n",
    "# Put your HF Token here\n",
    "#os.environ['HF_TOKEN']=\"hf_xxxxxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the dataset into inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'get_stock_price', 'description': 'Get the current stock price of a company', 'parameters': {'type': 'object', 'properties': {'company': {'type': 'string', 'description': 'The name of the company'}}, 'required': ['company']}}}, {'type': 'function', 'function': {'name': 'get_movie_details', 'description': 'Get details about a movie', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the movie'}}, 'required': ['title']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>\",\n",
       "   'role': 'system'},\n",
       "  {'content': 'Hi, can you tell me the current stock price of Apple?',\n",
       "   'role': 'human'},\n",
       "  {'content': \"<think>Alright, the user asked for the current stock price of Apple. Looking at the tools available, there's a function called get_stock_price which exactly fits this request. I'll need to call this function with the parameter company set to 'Apple' to retrieve the information the user is asking for. This seems like the most straightforward and appropriate action to take.\\n</think><tool_call>\\n{'name': 'get_stock_price', 'arguments': {'company': 'Apple'}}\\n</tool_call>\",\n",
       "   'role': 'model'},\n",
       "  {'content': \"<tool_response>\\n{'stock_price': '$150.75'}\\n</tool_response>\",\n",
       "   'role': 'tool'},\n",
       "  {'content': 'The current stock price of Apple is $150.75.', 'role': 'model'},\n",
       "  {'content': \"That's great. Can you also tell me the stock price for Microsoft?\",\n",
       "   'role': 'human'},\n",
       "  {'content': \"<think>Alright, let me try to figure out how to handle this user's request. The user first asked about Apple's stock price, and I responded by calling the get_stock_price function with 'Apple' as the argument. The tool provided the price, and I passed that information back to the user.\\n\\nNow, the user is asking for the stock price of Microsoft. I need to determine the appropriate action here. Looking at the tools available, there's still the get_stock_price function, which is designed to fetch current stock prices. Since the user is requesting another company's stock price, it makes sense to use the same function again but with the company name changed to Microsoft. \\n\\nI shouldn't assume any other tools are needed because the available functions are specifically tailored for stock prices and movie details. In this case, the relevant function is get_stock_price. I should ensure that the function is called with the correct parameter, 'Microsoft' in this case, to retrieve the accurate information the user is seeking. This approach is efficient and aligns directly with the user's request without making unnecessary assumptions or using unrelated tools.\\n</think><tool_call>\\n{'name': 'get_stock_price', 'arguments': {'company': 'Microsoft'}}\\n</tool_call>\",\n",
       "   'role': 'model'},\n",
       "  {'content': \"<tool_response>\\n{'stock_price': '$210.22'}\\n</tool_response>\",\n",
       "   'role': 'tool'},\n",
       "  {'content': 'The current stock price of Microsoft is $210.22.',\n",
       "   'role': 'model'},\n",
       "  {'content': 'Thank you for the information.', 'role': 'human'},\n",
       "  {'content': \"You're welcome! If you have any other questions, feel free to ask.\",\n",
       "   'role': 'model'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"google/gemma-2-2b-it\"\n",
    "dataset_name = \"Jofthomas/hermes-function-calling-thinking-V1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "\n",
    "def preprocess(sample):\n",
    "    messages = sample['messages']\n",
    "    first_message = messages[0]\n",
    "    if first_message['role'] == 'system':\n",
    "        system_message_content = first_message['content']\n",
    "        messages[1][\"content\"] = system_message_content + \"Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\n\" + messages[1][\"content\"]\n",
    "        messages.pop(0)\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    }\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset.rename_column(\"conversations\", \"messages\")\n",
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Dedicated Dataset for this unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3570/3570 [00:00<00:00, 11713.91 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3213\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 357\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "    preprocess, remove_columns=\"messages\"\n",
    ")\n",
    "dataset = dataset['train'].train_test_split(0.1)\n",
    "print(dataset)\n",
    "\n",
    "dataset['train'] = dataset['train'].select(range(100))\n",
    "dataset['test'] = dataset['test'].select(range(10))\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>human\n",
      "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'get_news_headlines', 'description': 'Get the latest news headlines', 'parameters': {'type': 'object', 'properties': {'country': {'type': 'string', 'description': 'The country for which headlines are needed'}}, 'required': ['country']}}}, {'type': 'function', 'function': {'name': 'search_recipes', 'description': 'Search for recipes based on ingredients', 'parameters': {'type': 'object', 'properties': {'ingredients': {'type': 'array', 'items': {'type': 'string'}, 'description': 'The list of ingredients'}}, 'required': ['ingredients']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "<tool_call>\n",
      "{tool_call}\n",
      "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
      "\n",
      "Can you get me the latest news headlines for the United States?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "<think>Okay, so the user asked for the latest news headlines in the United States. I need to figure out how to respond appropriately using the tools available.\n",
      "\n",
      "Looking at the tools provided, there's a function called get_news_headlines. That seems perfect for this situation. It requires a country code, which the user specified as the United States.\n",
      "\n",
      "Since the user is asking specifically about the US, I'll use the country parameter as 'United States' in the function call. This should fetch the relevant news for them.\n",
      "\n",
      "I should make sure to format the response correctly as a JSON object with the function name and arguments.\n",
      "</think><tool_call>\n",
      "{'name': 'get_news_headlines', 'arguments': {'country': 'United States'}}\n",
      "</tool_call><end_of_turn><eos>\n",
      "<start_of_turn>tool\n",
      "<tool_response>\n",
      "{'headlines': ['US President signs new bill into law', 'Major earthquake hits California', 'NASA announces new Mars mission', 'Tech giant releases new smartphone', 'US economy shows signs of recovery']}\n",
      "</tool_response><end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "Here are the latest news headlines for the United States:\n",
      "1. US President signs new bill into law\n",
      "2. Major earthquake hits California\n",
      "3. NASA announces new Mars mission\n",
      "4. Tech giant releases new smartphone\n",
      "5. US economy shows signs of recovery<end_of_turn><eos>\n",
      "<start_of_turn>human\n",
      "That's interesting. Can you get me the news for France now?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "<think>Okay, the user initially asked for the latest US news, so I used get_news_headlines with 'United States'. Now the user is asking for France, so I'll call the same function with 'France' as the country. That makes sense because the function is designed for any country.\n",
      "</think><tool_call>\n",
      "{'name': 'get_news_headlines', 'arguments': {'country': 'France'}}\n",
      "</tool_call><end_of_turn><eos>\n",
      "<start_of_turn>tool\n",
      "<tool_response>\n",
      "{'headlines': ['French President announces new environmental policy', 'Paris Fashion Week highlights', 'France wins World Cup qualifier', 'New culinary trend sweeps across France', 'French tech startup raises millions in funding']}\n",
      "</tool_response><end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "Here are the latest news headlines for France:\n",
      "1. French President announces new environmental policy\n",
      "2. Paris Fashion Week highlights\n",
      "3. France wins World Cup qualifier\n",
      "4. New culinary trend sweeps across France\n",
      "5. French tech startup raises millions in funding<end_of_turn><eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    dataset['train'][8]['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: <pad>, eos_token: <eos>\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"pad_token: {tokenizer.pad_token}, eos_token: {tokenizer.eos_token}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Modify the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [03:23<00:00, 101.93s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.49s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     27\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     28\u001b[39m     model_name,\n\u001b[32m     29\u001b[39m     attn_implementation=\u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     31\u001b[39m )\n\u001b[32m     33\u001b[39m model.resize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Playground/study-group-hf-agents-course/.venv/lib/python3.11/site-packages/accelerate/big_modeling.py:462\u001b[39m, in \u001b[36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model.parameters():\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m param.device == torch.device(\u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[31mRuntimeError\u001b[39m: You can't move a model that has some modules offloaded to cpu or disk."
     ]
    }
   ],
   "source": [
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    tools = \"<tools>\"\n",
    "    eotools = \"</tools>\"\n",
    "    think = \"<think>\"\n",
    "    eothink = \"</think>\"\n",
    "    tool_call = \"<tool_call>\"\n",
    "    eotool_call = \"</tool_call>\"\n",
    "    tool_response = \"<tool_response>\"\n",
    "    eotool_response = \"</tool_response>\"\n",
    "    pad_token = \"<pad>\"\n",
    "    eos_token = \"<eos>\"\n",
    "    \n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [\n",
    "            c.value\n",
    "            for c in cls\n",
    "        ]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    pad_token=ChatmlSpecialTokens.pad_token.value,\n",
    "    additional_special_tokens=ChatmlSpecialTokens.list()\n",
    ")\n",
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    attn_implementation=\"eager\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Configure the LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "rank_dimension = 16\n",
    "lora_alpha = 64\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=[\n",
    "        \"gate_proj\",\"q_proj\",\"lm_head\",\"o_proj\",\n",
    "        \"k_proj\",\"embed_tokens\",\"down_proj\",\"up_proj\",\"v_proj\"\n",
    "    ],\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets define the Trainer and the Fine Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"gemma-2-2B-it-thinking-function_calling-V0\"\n",
    "\n",
    "per_device_train_batch_size = 1\n",
    "per_device_eval_batch_size = 1\n",
    "gradient_accumulation_steps = 4\n",
    "logging_steps = 5\n",
    "learning_rate = 1e-4\n",
    "\n",
    "max_grad_norm = 1.0\n",
    "num_train_epochs=1\n",
    "warmup_ratio = 0.1\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_seq_length = 1500\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    "    bf16=True,\n",
    "    hub_private_repo=False,\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    packing=True,\n",
    "    max_seq_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"test\"][8][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"<bos><start_of_turn>human\n",
    "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'calculate_distance', 'description': 'Calculate the distance between two locations', 'parameters': {'type': 'object', 'properties': {'start_location': {'type': 'string', 'description': 'The starting location'}, 'end_location': {'type': 'string', 'description': 'The ending location'}}, 'required': ['start_location', 'end_location']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
    "<tool_call>\n",
    "{tool_call}\n",
    "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
    "\n",
    "Hi, I need to convert 500 USD to Euros. Can you help me with that?<end_of_turn><eos>\n",
    "<start_of_turn>model\n",
    "<think>\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False\n",
    ")\n",
    "inputs = {k: v.to(\"cuda\") for k,v in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=300,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.0,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
