{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Introduction to the LlamaHub](https://huggingface.co/learn/agents-course/unit2/llama-index/llama-hub#introduction-to-the-llamahub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"qwen2.5-coder:0.5b\")\n",
    "\n",
    "response = llm.complete(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Components in LlamaIndex](https://huggingface.co/learn/agents-course/unit2/llama-index/components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the persona database\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "dataset = load_dataset(path=\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
    "\n",
    "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i, persona in enumerate(dataset):\n",
    "    with open(Path(\"data\") / f\"persona_{i}.txt\", \"w\") as f:\n",
    "        f.write(persona[\"persona\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 documents\n",
      "Generated 10 nodes\n"
     ]
    }
   ],
   "source": [
    "# Loading and embedding persona documents\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir='data'\n",
    ")\n",
    "documents = reader.load_data()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "    \n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_overlap=0),\n",
    "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "nodes = await pipeline.arun(documents=documents[:10])\n",
    "print(f\"Generated {len(nodes)} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "# Storing and indexing documents\n",
    "\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(name=\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection)\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(),\n",
    "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "nodes = await pipeline.arun(documents=documents[:10])\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The persona described is a web developer who has a strong focus on HTML, CSS, and website building.\n"
     ]
    }
   ],
   "source": [
    "# Querying a VectorStoreIndex with prompts and LLMs\n",
    "\n",
    "llm = Ollama(model=\"qwen2.5-coder:0.5b\")\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"Respond using a persona that describes author and travel experiences?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Evaluation & Observability\n",
    "\n",
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "\n",
    "eval_result = evaluator.evaluate_response(response=response)\n",
    "print(eval_result.passing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Using Tools in LlamaIndex](https://huggingface.co/learn/agents-course/unit2/llama-index/tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolMetadata(description='Useful for getting the weather for a given location', name='my_weather_tool', fn_schema=<class 'llama_index.core.tools.utils.my_weather_tool'>, return_direct=False)\n",
      "Getting weather for New York\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='The weather in New York is sunny', tool_name='my_weather_tool', raw_input={'args': ('New York',), 'kwargs': {}}, raw_output='The weather in New York is sunny', is_error=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a Function Tool\n",
    "\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Useful for getting the weather for a given location\"\"\"\n",
    "    print(f\"Getting weather for {location}\")\n",
    "    return f\"The weather in {location} is sunny\"\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name=\"my_weather_tool\",\n",
    "    description=\"Useful for getting the weather for a given location\",\n",
    ")\n",
    "\n",
    "print(tool.metadata)\n",
    "\n",
    "tool.call(\"New York\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/Playground/study-group-hf-agents-course/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolMetadata(description='Useful for querying the alfred index', name='alfred_query_engine', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='Research on the impact of AI on the future of work and society.', tool_name='alfred_query_engine', raw_input={'input': 'Responds about research on the impact of AI on the future of work and society?'}, raw_output=Response(response='Research on the impact of AI on the future of work and society.', source_nodes=[NodeWithScore(node=TextNode(id_='1a4731fd-7dda-4e47-a48b-681f6b092529', embedding=None, metadata={'file_path': '/Users/pardeep/Playground/study-group-hf-agents-course/units/unit2_frameworks/llamaindex/data/persona_100.txt', 'file_name': 'persona_100.txt', 'file_type': 'text/plain', 'file_size': 107, 'creation_date': '2025-06-28', 'last_modified_date': '2025-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d53cee97-50bd-45de-b7ea-033ed0c0faea', node_type='4', metadata={'file_path': '/Users/pardeep/Playground/study-group-hf-agents-course/units/unit2_frameworks/llamaindex/data/persona_100.txt', 'file_name': 'persona_100.txt', 'file_type': 'text/plain', 'file_size': 107, 'creation_date': '2025-06-28', 'last_modified_date': '2025-06-28'}, hash='0e2a660bacf6d836654d8441793ca88516503b3df1ce83700d5341ae76369bd6')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='An environmental conservationist focused on wetland ecosystems and their role in mitigating climate change.', mimetype='text/plain', start_char_idx=0, end_char_idx=107, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.3749921375643614), NodeWithScore(node=TextNode(id_='5dba7bbd-98e6-4fd9-b2e3-cc5adebce1ee', embedding=None, metadata={'file_path': '/Users/pardeep/Playground/study-group-hf-agents-course/units/unit2_frameworks/llamaindex/data/persona_10.txt', 'file_name': 'persona_10.txt', 'file_type': 'text/plain', 'file_size': 207, 'creation_date': '2025-06-28', 'last_modified_date': '2025-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ff0f0d5e-a785-409f-8d6a-3aff498367d5', node_type='4', metadata={'file_path': '/Users/pardeep/Playground/study-group-hf-agents-course/units/unit2_frameworks/llamaindex/data/persona_10.txt', 'file_name': 'persona_10.txt', 'file_type': 'text/plain', 'file_size': 207, 'creation_date': '2025-06-28', 'last_modified_date': '2025-06-28'}, hash='49752ea18d374964dbe8cc7ea92b6d6322e641786e48b71e084e88f0d144d427')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='A social justice educator or activist focused on diversity, equity, and inclusion, likely working with families and communities to promote empathy and understanding of intersectional identity and oppression.', mimetype='text/plain', start_char_idx=0, end_char_idx=207, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.3744665360993362)], metadata={'1a4731fd-7dda-4e47-a48b-681f6b092529': {'file_path': '/Users/pardeep/Playground/study-group-hf-agents-course/units/unit2_frameworks/llamaindex/data/persona_100.txt', 'file_name': 'persona_100.txt', 'file_type': 'text/plain', 'file_size': 107, 'creation_date': '2025-06-28', 'last_modified_date': '2025-06-28'}, '5dba7bbd-98e6-4fd9-b2e3-cc5adebce1ee': {'file_path': '/Users/pardeep/Playground/study-group-hf-agents-course/units/unit2_frameworks/llamaindex/data/persona_10.txt', 'file_name': 'persona_10.txt', 'file_type': 'text/plain', 'file_size': 207, 'creation_date': '2025-06-28', 'last_modified_date': '2025-06-28'}}), is_error=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a QueryEngineTool\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(name=\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "llm = Ollama(model=\"qwen2.5-coder:0.5b\")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"alfred_query_engine\",\n",
    "    description=\"Useful for querying the alfred index\",\n",
    ")\n",
    "\n",
    "print(tool.metadata)\n",
    "\n",
    "await tool.acall(\n",
    "    \"Responds about research on the impact of AI on the future of work and society?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.core.tools.function_tool.FunctionTool object at 0x327f13ed0>, <llama_index.core.tools.function_tool.FunctionTool object at 0x326f44850>, <llama_index.core.tools.function_tool.FunctionTool object at 0x160008c10>, <llama_index.core.tools.function_tool.FunctionTool object at 0x326f28890>, <llama_index.core.tools.function_tool.FunctionTool object at 0x326f89910>, <llama_index.core.tools.function_tool.FunctionTool object at 0x326f28210>]\n",
      "tool.metadata.name: load_data\n",
      "tool.metadata.description: load_data() -> List[llama_index.core.schema.Document]\n",
      "Load emails from the user's account.\n",
      "tool.metadata.name: search_messages\n",
      "tool.metadata.description: search_messages(query: str, max_results: Optional[int] = None)\n",
      "\n",
      "        Searches email messages given a query string and the maximum number\n",
      "        of results requested by the user\n",
      "           Returns: List of relevant message objects up to the maximum number of results.\n",
      "\n",
      "        Args:\n",
      "            query (str): The user's query\n",
      "            max_results (Optional[int]): The maximum number of search results\n",
      "            to return.\n",
      "\n",
      "        \n",
      "tool.metadata.name: create_draft\n",
      "tool.metadata.description: create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\n",
      "\n",
      "        Create and insert a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            to (Optional[str]): The email addresses to send the message to\n",
      "            subject (Optional[str]): The subject for the event\n",
      "            message (Optional[str]): The message for the event\n",
      "\n",
      "        \n",
      "tool.metadata.name: update_draft\n",
      "tool.metadata.description: update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\n",
      "\n",
      "        Update a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           This function is required to be passed a draft_id that is obtained when creating messages\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            to (Optional[str]): The email addresses to send the message to\n",
      "            subject (Optional[str]): The subject for the event\n",
      "            message (Optional[str]): The message for the event\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "\n",
      "        \n",
      "tool.metadata.name: get_draft\n",
      "tool.metadata.description: get_draft(draft_id: str = None) -> str\n",
      "\n",
      "        Get a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "\n",
      "        \n",
      "tool.metadata.name: send_draft\n",
      "tool.metadata.description: send_draft(draft_id: str = None) -> str\n",
      "\n",
      "        Sends a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Creating ToolSpecs\n",
    "\n",
    "from llama_index.tools.google import GmailToolSpec\n",
    "\n",
    "tool_spec = GmailToolSpec()\n",
    "tool_spec_list = tool_spec.to_tool_list()\n",
    "print(tool_spec_list)\n",
    "\n",
    "for tool in tool_spec_list:\n",
    "    print(\n",
    "        f\"tool.metadata.name: {tool.metadata.name}\\n\"\n",
    "        f\"tool.metadata.description: {tool.metadata.description}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
